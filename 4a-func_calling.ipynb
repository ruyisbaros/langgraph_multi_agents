{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END, MessageGraph\n",
    "from IPython.display import display, Image\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "llm = ChatGroq(\n",
    "    # model=\"llama3-groq-70b-8192-tool-use-preview\",\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_weather(location:str)->str:\n",
    "    \"\"\"\n",
    "    Get the current weather in a specific location.\n",
    "    This tool simulates checking the weather by randomly selecting from three possible outcomes: Sunny, cold or rainy.\n",
    "    The chance of each outcome is equal (1/3). If the random check fails, it may return an unexpected result to simulate.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location for which the weather is to be checked.\n",
    "        \n",
    "    Returns:\n",
    "        str: A string describing the weather in the given location, randomly chosen from three possible outcomes.\n",
    "    \"\"\"\n",
    "    if random.randint(0, 2) == 0:\n",
    "        return \"Sunny, 78F\"\n",
    "    elif random.randint(0, 2) == 1:\n",
    "        return \"Cold, 22F\"\n",
    "    else:\n",
    "        return \"Rainy, 65F\"\n",
    "    \n",
    "\n",
    "@tool\n",
    "def get_system_time(location:str=\"Berlin\")->str:\n",
    "    \"\"\"\n",
    "    Get the system time. If no location is provided use the default location as \"Minesota\"\n",
    "    This tool simulates retrieving the system time by randomly selecting from three possible outcomes: morning, afternoon, evenning.\n",
    "    The chance of each outcome is equal (1/3). If the random check fails, it may return an unexpected result to simulate.\n",
    "    \n",
    "    Args:\n",
    "        location (str, optional): The location for which the time is to be retrieved. Defaults to \"Minesota\".\n",
    "        \n",
    "    Returns:\n",
    "        str: A string describing the time of day, randomly chosen from three possible outcomes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if random.randint(0, 2) == 0:\n",
    "        return \"2:00 AM\"\n",
    "    elif random.randint(0, 2) == 1:\n",
    "        return \"3:00 PM\"\n",
    "    else:\n",
    "        return \"6:00 PM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_weather, get_system_time]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "tool_mapping = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"get_system_time\": get_system_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_qvqj', 'function': {'arguments': '{\"location\": \"Woodbury, MN\"}', 'name': 'get_current_weather'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 737, 'total_tokens': 755, 'completion_time': 0.072, 'prompt_time': 0.155655706, 'queue_time': 0.21504934199999998, 'total_time': 0.227655706}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-d9e2422e-c751-4d59-8cd7-80c8377101c3-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Woodbury, MN'}, 'id': 'call_qvqj', 'type': 'tool_call'}] usage_metadata={'input_tokens': 737, 'output_tokens': 18, 'total_tokens': 755}\n"
     ]
    }
   ],
   "source": [
    "# Define prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"system\n",
    "    You are a smart Agent. \n",
    "    You are a master at understanding what a customer wants and utilize available tools only if you have to.\n",
    "\n",
    "    user\n",
    "    Conduct a comprehensive analysis of the request provided\\\n",
    "\n",
    "    USER REQUEST:\\n\\n {initial_request} \\n\\n\n",
    "    \n",
    "    assistant\n",
    "    \"\"\",\n",
    "    input_variables=[\"initial_request\"],\n",
    ")\n",
    "\n",
    "agent_request_generator = prompt | llm_with_tools\n",
    "\n",
    "result = agent_request_generator.invoke(\n",
    "    {\"initial_request\": \"What is the weather in woodbury in MN?\"}\n",
    "    )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_ai_message: content='' additional_kwargs={'tool_calls': [{'id': 'call_qvqj', 'function': {'arguments': '{\"location\": \"Woodbury, MN\"}', 'name': 'get_current_weather'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 737, 'total_tokens': 755, 'completion_time': 0.072, 'prompt_time': 0.155655706, 'queue_time': 0.21504934199999998, 'total_time': 0.227655706}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-d9e2422e-c751-4d59-8cd7-80c8377101c3-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Woodbury, MN'}, 'id': 'call_qvqj', 'type': 'tool_call'}] usage_metadata={'input_tokens': 737, 'output_tokens': 18, 'total_tokens': 755}\n",
      "tool name ::  get_current_weather\n",
      "tool args ::  {'location': 'Woodbury, MN'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the last AI message from messages\n",
    "last_ai_message = None\n",
    "if isinstance(result, AIMessage):\n",
    "    last_ai_message = result\n",
    "\n",
    "# print(\"Last AI Message:\", last_ai_message)\n",
    "\n",
    "if last_ai_message and hasattr(last_ai_message, 'tool_calls'):\n",
    "    print(\"last_ai_message:\", last_ai_message)\n",
    "    print(\"tool name :: \", last_ai_message.tool_calls[-1][\"name\"])\n",
    "    print(\"tool args :: \", last_ai_message.tool_calls[-1][\"args\"])\n",
    "else:\n",
    "    print(\"No tool calls found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_78yz', 'function': {'arguments': '{\"location\": \"Woodbury, MN\"}', 'name': 'get_system_time'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 737, 'total_tokens': 755, 'completion_time': 0.072, 'prompt_time': 0.136998677, 'queue_time': 0.003663271999999995, 'total_time': 0.208998677}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-aa08916b-7b61-491b-896b-678951680ed4-0' tool_calls=[{'name': 'get_system_time', 'args': {'location': 'Woodbury, MN'}, 'id': 'call_78yz', 'type': 'tool_call'}] usage_metadata={'input_tokens': 737, 'output_tokens': 18, 'total_tokens': 755}\n"
     ]
    }
   ],
   "source": [
    "result = agent_request_generator.invoke(\n",
    "    {\"initial_request\": \"What is the time in woodbury in MN?\"}\n",
    "    )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_ai_message: content='' additional_kwargs={'tool_calls': [{'id': 'call_78yz', 'function': {'arguments': '{\"location\": \"Woodbury, MN\"}', 'name': 'get_system_time'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 737, 'total_tokens': 755, 'completion_time': 0.072, 'prompt_time': 0.136998677, 'queue_time': 0.003663271999999995, 'total_time': 0.208998677}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-aa08916b-7b61-491b-896b-678951680ed4-0' tool_calls=[{'name': 'get_system_time', 'args': {'location': 'Woodbury, MN'}, 'id': 'call_78yz', 'type': 'tool_call'}] usage_metadata={'input_tokens': 737, 'output_tokens': 18, 'total_tokens': 755}\n",
      "tool name ::  get_system_time\n",
      "tool args ::  {'location': 'Woodbury, MN'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the last AI message from messages\n",
    "last_ai_message = None\n",
    "if isinstance(result, AIMessage):\n",
    "    last_ai_message = result\n",
    "\n",
    "# print(\"Last AI Message:\", last_ai_message)\n",
    "\n",
    "if last_ai_message and hasattr(last_ai_message, 'tool_calls'):\n",
    "    print(\"last_ai_message:\", last_ai_message)\n",
    "    print(\"tool name :: \", last_ai_message.tool_calls[-1][\"name\"])\n",
    "    print(\"tool args :: \", last_ai_message.tool_calls[-1][\"args\"])\n",
    "else:\n",
    "    print(\"No tool calls found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
