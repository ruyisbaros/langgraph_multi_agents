{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END, MessageGraph\n",
    "from IPython.display import display, Image\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "llm = ChatGroq(\n",
    "    # model=\"llama3-groq-70b-8192-tool-use-preview\",\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Schema for structured response\n",
    "class Evaluation(BaseModel):\n",
    "    result: bool = Field(description=\"True or False\", required=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template llama3\n",
    "category_generator_prompt = PromptTemplate(\n",
    "    template=\n",
    "    \"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "        You are a Smart Router Agent. You are a master at reviewing whether the original question that customer asked was answered in the tool response.\n",
    "        You understand the context and question below and return your answer in JSON.\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    CONTEXT: Conduct a comprehensive analysis of the Initial Request from user and Tool Response and route the request into boolean true or false:\n",
    "        True - used when INITIAL REQUEST appears to be answered by TOOL RESPONSE. \\\n",
    "        False - used when INITIAL REQUEST is not answered by TOOL RESPONSE or when TOOL RESPONSE is empty \\\n",
    "\n",
    "            Output either True or False \\\n",
    "            eg:\n",
    "            'True' \\n\\n\n",
    "    INITIAL REQUEST:\\n\\n {research_question} \\n\\n\n",
    "    TOOL RESPONSE:\\n\\n {tool_response} \\n\\n\n",
    "\n",
    "    JSON:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    " input_variables=[\"research_question\", \"tool_response\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(Evaluation)\n",
    "category_generator = category_generator_prompt | structured_llm\n",
    "\n",
    "# result = category_generator.invoke({\n",
    "#     \"research_question\": \"What is 2+2\",\n",
    "#     \"tool_response\": \"4\"\n",
    "#     })\n",
    "\n",
    "result = category_generator.invoke({\n",
    "    \"research_question\": \"What is the weather in woodbury in MN?\",\n",
    "    \"tool_response\":\"65F, Sunny\"})\n",
    "\n",
    "\n",
    "print(result.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
