{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END, MessageGraph\n",
    "from IPython.display import display, Image\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "llm = ChatGroq(\n",
    "    # model=\"llama3-groq-70b-8192-tool-use-preview\",\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools from tools.py\n",
    "from tools import get_current_weather, get_system_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = llm.bind_tools(\n",
    "    tools=[get_current_weather, get_system_time],\n",
    ")\n",
    "\n",
    "tool_mapping = {\n",
    "    'get_current_weather': get_current_weather,\n",
    "    'get_system_time': get_system_time,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\n",
    "    \"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "        You are a smart Agent. \n",
    "        You are a master at understanding what a customer wants and utilize available tools only if you have to.\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "        Conduct a comprehensive analysis of the request provided. \\n\n",
    "        USER REQUEST:\\n\\n {initial_request} \\n\\n\n",
    "\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"initial_request\"],\n",
    ")\n",
    "\n",
    "agent_request_generator = prompt | model_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_nc4k', 'function': {'arguments': '{\"location\": \"Woodbury, MN\"}', 'name': 'get_current_weather'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 833, 'total_tokens': 851, 'completion_time': 0.072, 'prompt_time': 0.160661552, 'queue_time': 0.14008964799999998, 'total_time': 0.232661552}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-003294fc-1a4e-42c5-81e5-f2b81ea04949-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Woodbury, MN'}, 'id': 'call_nc4k', 'type': 'tool_call'}] usage_metadata={'input_tokens': 833, 'output_tokens': 18, 'total_tokens': 851}\n"
     ]
    }
   ],
   "source": [
    "result = agent_request_generator.invoke(\n",
    "    {\"initial_request\": \"What is the weather in woodbury in MN?\"}\n",
    "    )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_ai_message: content='' additional_kwargs={'tool_calls': [{'id': 'call_nc4k', 'function': {'arguments': '{\"location\": \"Woodbury, MN\"}', 'name': 'get_current_weather'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 833, 'total_tokens': 851, 'completion_time': 0.072, 'prompt_time': 0.160661552, 'queue_time': 0.14008964799999998, 'total_time': 0.232661552}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-003294fc-1a4e-42c5-81e5-f2b81ea04949-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Woodbury, MN'}, 'id': 'call_nc4k', 'type': 'tool_call'}] usage_metadata={'input_tokens': 833, 'output_tokens': 18, 'total_tokens': 851}\n",
      "tool name ::  get_current_weather\n",
      "tool args ::  {'location': 'Woodbury, MN'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the last AI message from messages\n",
    "last_ai_message = None\n",
    "if isinstance(result, AIMessage):\n",
    "    last_ai_message = result\n",
    "\n",
    "#print(\"Last AI Message:\", last_ai_message)\n",
    "\n",
    "if last_ai_message and hasattr(last_ai_message, 'tool_calls'):\n",
    "    print(\"last_ai_message:\", last_ai_message)\n",
    "    print(\"tool name :: \", last_ai_message.tool_calls[-1][\"name\"])\n",
    "    print(\"tool args :: \", last_ai_message.tool_calls[-1][\"args\"])\n",
    "else:\n",
    "    print(\"No tool calls found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunny, 78F\n"
     ]
    }
   ],
   "source": [
    "#run tool\n",
    "for tool_call in result.tool_calls:\n",
    "    tool = tool_mapping[tool_call[\"name\"].lower()]\n",
    "    tool_output = tool.invoke(tool_call[\"args\"])\n",
    "    print(tool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
